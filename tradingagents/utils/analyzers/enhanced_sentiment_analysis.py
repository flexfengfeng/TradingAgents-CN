#!/usr/bin/env python3
"""
å¢å¼ºæ–°é—»æƒ…ç»ªåˆ†æå™¨
å…ˆé‡åŒ–è®¡ç®—æƒ…ç»ªè¯„åˆ†å’Œå½±å“æƒé‡ï¼Œç„¶åäº¤ç»™DeepSeekè¿›è¡Œæ·±åº¦åˆ†æ
è§£å†³DeepSeekåœ¨æƒ…ç»ªé‡åŒ–ä¸Šä¸å¤Ÿç²¾ç¡®çš„é—®é¢˜
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List, Tuple
import re
import json
from collections import defaultdict
import math


class EnhancedSentimentAnalyzer:
    """
    å¢å¼ºæ–°é—»æƒ…ç»ªåˆ†æå™¨
    åŠŸèƒ½ï¼šå…ˆé‡åŒ–è®¡ç®—æƒ…ç»ªè¯„åˆ†å’Œå½±å“æƒé‡ï¼Œå†æ ¼å¼åŒ–ä¸ºé€‚åˆLLMåˆ†æçš„æŠ¥å‘Š
    """
    
    def __init__(self):
        self.current_date = datetime.now().strftime('%Y-%m-%d')
        self.sentiment_keywords = self._load_sentiment_keywords()
        self.impact_weights = self._load_impact_weights()
        print("ğŸ“° å¢å¼ºæ–°é—»æƒ…ç»ªåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _load_sentiment_keywords(self) -> Dict[str, Dict[str, float]]:
        """åŠ è½½æƒ…ç»ªå…³é”®è¯è¯å…¸"""
        return {
            "positive": {
                "ä¸Šæ¶¨": 0.8, "æ¶¨åœ": 1.0, "çªç ´": 0.7, "åˆ›æ–°é«˜": 0.9,
                "åˆ©å¥½": 0.8, "ç›ˆåˆ©": 0.6, "å¢é•¿": 0.7, "æ‰©å¼ ": 0.6,
                "åˆä½œ": 0.5, "ç­¾çº¦": 0.6, "ä¸­æ ‡": 0.7, "è·å¾—": 0.5,
                "æˆåŠŸ": 0.6, "ä¼˜ç§€": 0.5, "å¼ºåŠ²": 0.7, "è¶…é¢„æœŸ": 0.8,
                "ä¹°å…¥": 0.6, "æ¨è": 0.5, "çœ‹å¥½": 0.6, "ä¹è§‚": 0.6
            },
            "negative": {
                "ä¸‹è·Œ": -0.8, "è·Œåœ": -1.0, "æš´è·Œ": -0.9, "ç ´ä½": -0.7,
                "åˆ©ç©º": -0.8, "äºæŸ": -0.7, "ä¸‹æ»‘": -0.6, "èç¼©": -0.6,
                "é£é™©": -0.6, "è­¦å‘Š": -0.7, "è°ƒæŸ¥": -0.5, "å¤„ç½š": -0.8,
                "å¤±è´¥": -0.6, "å›°éš¾": -0.5, "ç–²è½¯": -0.6, "ä½äºé¢„æœŸ": -0.7,
                "å–å‡º": -0.6, "å‡æŒ": -0.5, "çœ‹ç©º": -0.6, "æ‚²è§‚": -0.6
            },
            "neutral": {
                "å…¬å‘Š": 0.0, "æŠ«éœ²": 0.0, "å‘å¸ƒ": 0.0, "å¬å¼€": 0.0,
                "ä¼šè®®": 0.0, "è®¨è®º": 0.0, "åˆ†æ": 0.0, "ç ”ç©¶": 0.0
            }
        }
    
    def _load_impact_weights(self) -> Dict[str, float]:
        """åŠ è½½å½±å“æƒé‡é…ç½®"""
        return {
            "news_source": {
                "å¤®è§†æ–°é—»": 1.0, "æ–°åç¤¾": 1.0, "äººæ°‘æ—¥æŠ¥": 1.0,
                "è¯åˆ¸æ—¶æŠ¥": 0.9, "ä¸Šæµ·è¯åˆ¸æŠ¥": 0.9, "ä¸­å›½è¯åˆ¸æŠ¥": 0.9,
                "è´¢ç»": 0.8, "ç¬¬ä¸€è´¢ç»": 0.8, "21ä¸–çºªç»æµæŠ¥é“": 0.8,
                "æ–°æµªè´¢ç»": 0.7, "è…¾è®¯è´¢ç»": 0.7, "ç½‘æ˜“è´¢ç»": 0.7,
                "å…¶ä»–": 0.5
            },
            "news_type": {
                "æ”¿ç­–": 1.0, "ç›‘ç®¡": 0.9, "ä¸šç»©": 0.9, "é‡ç»„": 0.8,
                "åˆä½œ": 0.7, "äº§å“": 0.6, "äººäº‹": 0.5, "å…¶ä»–": 0.4
            },
            "time_decay": {
                "å½“æ—¥": 1.0, "1å¤©å‰": 0.9, "2å¤©å‰": 0.8, "3å¤©å‰": 0.7,
                "1å‘¨å‰": 0.6, "2å‘¨å‰": 0.4, "1æœˆå‰": 0.2, "æ›´æ—©": 0.1
            }
        }
    
    def analyze_news_sentiment(self, news_data: str, stock_code: str = None) -> Dict[str, Any]:
        """
        åˆ†ææ–°é—»æƒ…ç»ªæ•°æ®ï¼Œè®¡ç®—é‡åŒ–æŒ‡æ ‡
        
        Args:
            news_data: æ–°é—»æ•°æ®
            stock_code: è‚¡ç¥¨ä»£ç ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            åŒ…å«è®¡ç®—ç»“æœçš„å­—å…¸
        """
        try:
            print("ğŸ“° å¼€å§‹å¢å¼ºæ–°é—»æƒ…ç»ªåˆ†æ...")
            
            # è§£ææ–°é—»æ•°æ®
            news_items = self._parse_news_data(news_data)
            if not news_items:
                return {"error": "æ— æ³•è§£ææ–°é—»æ•°æ®"}
            
            # è®¡ç®—æƒ…ç»ªè¯„åˆ†
            sentiment_scores = self._calculate_sentiment_scores(news_items)
            
            # è®¡ç®—å½±å“æƒé‡
            impact_weights = self._calculate_impact_weights(news_items)
            
            # è®¡ç®—æ—¶é—´è¡°å‡
            time_decay_factors = self._calculate_time_decay(news_items)
            
            # è®¡ç®—ç»¼åˆæƒ…ç»ªæŒ‡æ ‡
            comprehensive_sentiment = self._calculate_comprehensive_sentiment(
                sentiment_scores, impact_weights, time_decay_factors
            )
            
            # æƒ…ç»ªè¶‹åŠ¿åˆ†æ
            sentiment_trend = self._analyze_sentiment_trend(news_items, sentiment_scores)
            
            # çƒ­ç‚¹è¯é¢˜åˆ†æ
            hot_topics = self._analyze_hot_topics(news_items)
            
            # å¸‚åœºå…³æ³¨åº¦åˆ†æ
            market_attention = self._analyze_market_attention(news_items)
            
            # é£é™©é¢„è­¦
            risk_alerts = self._generate_risk_alerts(comprehensive_sentiment, sentiment_trend)
            
            return {
                "news_count": len(news_items),
                "sentiment_scores": sentiment_scores,
                "impact_weights": impact_weights,
                "time_decay_factors": time_decay_factors,
                "comprehensive_sentiment": comprehensive_sentiment,
                "sentiment_trend": sentiment_trend,
                "hot_topics": hot_topics,
                "market_attention": market_attention,
                "risk_alerts": risk_alerts,
                "analysis_date": self.current_date,
                "data_quality": self._assess_news_data_quality(news_items)
            }
            
        except Exception as e:
            print(f"âŒ æ–°é—»æƒ…ç»ªåˆ†æå¤±è´¥: {e}")
            return {"error": f"æ–°é—»æƒ…ç»ªåˆ†æå¤±è´¥: {str(e)}"}
    
    def _parse_news_data(self, news_data: str) -> List[Dict[str, Any]]:
        """è§£ææ–°é—»æ•°æ®"""
        news_items = []
        
        try:
            lines = news_data.split('\n')
            current_news = {}
            
            for line in lines:
                line = line.strip()
                if not line:
                    if current_news:
                        news_items.append(current_news)
                        current_news = {}
                    continue
                
                # è§£ææ ‡é¢˜
                if line.startswith('æ ‡é¢˜:') or line.startswith('Title:'):
                    current_news['title'] = line.split(':', 1)[1].strip()
                
                # è§£æå†…å®¹
                elif line.startswith('å†…å®¹:') or line.startswith('Content:'):
                    current_news['content'] = line.split(':', 1)[1].strip()
                
                # è§£ææ—¶é—´
                elif line.startswith('æ—¶é—´:') or line.startswith('Time:'):
                    current_news['time'] = line.split(':', 1)[1].strip()
                
                # è§£ææ¥æº
                elif line.startswith('æ¥æº:') or line.startswith('Source:'):
                    current_news['source'] = line.split(':', 1)[1].strip()
                
                # è§£æURL
                elif line.startswith('é“¾æ¥:') or line.startswith('URL:'):
                    current_news['url'] = line.split(':', 1)[1].strip()
                
                # å¦‚æœæ²¡æœ‰æ˜ç¡®æ ‡è¯†ï¼Œå°è¯•æ™ºèƒ½è§£æ
                elif ':' in line and len(current_news) == 0:
                    # å¯èƒ½æ˜¯æ ‡é¢˜
                    current_news['title'] = line
                elif len(line) > 20 and 'content' not in current_news:
                    # å¯èƒ½æ˜¯å†…å®¹
                    current_news['content'] = line
            
            # æ·»åŠ æœ€åä¸€æ¡æ–°é—»
            if current_news:
                news_items.append(current_news)
            
            # ä¸ºç¼ºå¤±å­—æ®µè®¾ç½®é»˜è®¤å€¼
            for item in news_items:
                item.setdefault('title', 'æœªçŸ¥æ ‡é¢˜')
                item.setdefault('content', '')
                item.setdefault('time', self.current_date)
                item.setdefault('source', 'æœªçŸ¥æ¥æº')
                item.setdefault('url', '')
            
            return news_items
            
        except Exception as e:
            print(f"âŒ æ–°é—»æ•°æ®è§£æå¤±è´¥: {e}")
            return []
    
    def _calculate_sentiment_scores(self, news_items: List[Dict]) -> Dict[str, Any]:
        """è®¡ç®—æƒ…ç»ªè¯„åˆ†"""
        scores = {
            "individual_scores": [],
            "positive_count": 0,
            "negative_count": 0,
            "neutral_count": 0,
            "average_score": 0.0,
            "sentiment_distribution": {},
            "extreme_sentiment_ratio": 0.0
        }
        
        total_score = 0.0
        
        for item in news_items:
            text = f"{item.get('title', '')} {item.get('content', '')}"
            score = self._calculate_text_sentiment(text)
            scores["individual_scores"].append({
                "title": item.get('title', ''),
                "score": score,
                "sentiment": self._classify_sentiment(score)
            })
            
            total_score += score
            
            # ç»Ÿè®¡æƒ…ç»ªåˆ†å¸ƒ
            if score > 0.1:
                scores["positive_count"] += 1
            elif score < -0.1:
                scores["negative_count"] += 1
            else:
                scores["neutral_count"] += 1
        
        if news_items:
            scores["average_score"] = total_score / len(news_items)
        
        # è®¡ç®—æƒ…ç»ªåˆ†å¸ƒ
        total_count = len(news_items)
        if total_count > 0:
            scores["sentiment_distribution"] = {
                "positive_ratio": scores["positive_count"] / total_count,
                "negative_ratio": scores["negative_count"] / total_count,
                "neutral_ratio": scores["neutral_count"] / total_count
            }
        
        # è®¡ç®—æç«¯æƒ…ç»ªæ¯”ä¾‹
        extreme_count = sum(1 for item in scores["individual_scores"] 
                          if abs(item["score"]) > 0.7)
        scores["extreme_sentiment_ratio"] = extreme_count / total_count if total_count > 0 else 0
        
        return scores
    
    def _calculate_text_sentiment(self, text: str) -> float:
        """è®¡ç®—æ–‡æœ¬æƒ…ç»ªè¯„åˆ†"""
        score = 0.0
        word_count = 0
        
        # æ­£é¢æƒ…ç»ªè¯æ±‡
        for word, weight in self.sentiment_keywords["positive"].items():
            count = text.count(word)
            score += count * weight
            word_count += count
        
        # è´Ÿé¢æƒ…ç»ªè¯æ±‡
        for word, weight in self.sentiment_keywords["negative"].items():
            count = text.count(word)
            score += count * weight  # weightå·²ç»æ˜¯è´Ÿæ•°
            word_count += count
        
        # å½’ä¸€åŒ–å¤„ç†
        if word_count > 0:
            score = score / math.sqrt(word_count)  # ä½¿ç”¨å¹³æ–¹æ ¹å½’ä¸€åŒ–
        
        # é™åˆ¶åœ¨[-1, 1]èŒƒå›´å†…
        return max(-1.0, min(1.0, score))
    
    def _classify_sentiment(self, score: float) -> str:
        """åˆ†ç±»æƒ…ç»ª"""
        if score > 0.3:
            return "ç§¯æ"
        elif score < -0.3:
            return "æ¶ˆæ"
        else:
            return "ä¸­æ€§"
    
    def _calculate_impact_weights(self, news_items: List[Dict]) -> Dict[str, Any]:
        """è®¡ç®—å½±å“æƒé‡"""
        weights = {
            "individual_weights": [],
            "source_weights": {},
            "average_weight": 0.0,
            "high_impact_ratio": 0.0
        }
        
        total_weight = 0.0
        high_impact_count = 0
        
        for item in news_items:
            source = item.get('source', 'å…¶ä»–')
            title = item.get('title', '')
            
            # åŸºç¡€æƒé‡ï¼ˆåŸºäºæ¥æºï¼‰
            base_weight = self._get_source_weight(source)
            
            # å†…å®¹æƒé‡ï¼ˆåŸºäºå…³é”®è¯ï¼‰
            content_weight = self._get_content_weight(title + ' ' + item.get('content', ''))
            
            # ç»¼åˆæƒé‡
            final_weight = base_weight * content_weight
            
            weights["individual_weights"].append({
                "title": title,
                "source": source,
                "base_weight": base_weight,
                "content_weight": content_weight,
                "final_weight": final_weight
            })
            
            total_weight += final_weight
            
            if final_weight > 0.7:
                high_impact_count += 1
            
            # ç»Ÿè®¡æ¥æºæƒé‡åˆ†å¸ƒ
            if source not in weights["source_weights"]:
                weights["source_weights"][source] = []
            weights["source_weights"][source].append(final_weight)
        
        if news_items:
            weights["average_weight"] = total_weight / len(news_items)
            weights["high_impact_ratio"] = high_impact_count / len(news_items)
        
        # è®¡ç®—å„æ¥æºå¹³å‡æƒé‡
        for source in weights["source_weights"]:
            source_weights = weights["source_weights"][source]
            weights["source_weights"][source] = {
                "average": sum(source_weights) / len(source_weights),
                "count": len(source_weights)
            }
        
        return weights
    
    def _get_source_weight(self, source: str) -> float:
        """è·å–æ¥æºæƒé‡"""
        for key, weight in self.impact_weights["news_source"].items():
            if key in source:
                return weight
        return self.impact_weights["news_source"]["å…¶ä»–"]
    
    def _get_content_weight(self, content: str) -> float:
        """è·å–å†…å®¹æƒé‡"""
        weight = 1.0
        
        # åŸºäºå…³é”®è¯è°ƒæ•´æƒé‡
        high_impact_keywords = ["æ”¿ç­–", "ç›‘ç®¡", "ä¸šç»©", "é‡ç»„", "å¹¶è´­", "IPO", "é€€å¸‚"]
        for keyword in high_impact_keywords:
            if keyword in content:
                weight *= 1.2
        
        # åŸºäºæ•°å­—è°ƒæ•´æƒé‡ï¼ˆæ¶‰åŠå…·ä½“æ•°æ®çš„æ–°é—»é€šå¸¸æ›´é‡è¦ï¼‰
        if re.search(r'\d+%|\d+äº¿|\d+ä¸‡', content):
            weight *= 1.1
        
        return min(weight, 2.0)  # é™åˆ¶æœ€å¤§æƒé‡
    
    def _calculate_time_decay(self, news_items: List[Dict]) -> Dict[str, Any]:
        """è®¡ç®—æ—¶é—´è¡°å‡å› å­"""
        decay_factors = {
            "individual_factors": [],
            "average_decay": 0.0,
            "fresh_news_ratio": 0.0
        }
        
        current_time = datetime.now()
        total_decay = 0.0
        fresh_count = 0
        
        for item in news_items:
            time_str = item.get('time', self.current_date)
            decay_factor = self._calculate_single_time_decay(time_str, current_time)
            
            decay_factors["individual_factors"].append({
                "title": item.get('title', ''),
                "time": time_str,
                "decay_factor": decay_factor
            })
            
            total_decay += decay_factor
            
            if decay_factor > 0.8:  # è®¤ä¸ºæ˜¯æ–°é²œæ–°é—»
                fresh_count += 1
        
        if news_items:
            decay_factors["average_decay"] = total_decay / len(news_items)
            decay_factors["fresh_news_ratio"] = fresh_count / len(news_items)
        
        return decay_factors
    
    def _calculate_single_time_decay(self, time_str: str, current_time: datetime) -> float:
        """è®¡ç®—å•æ¡æ–°é—»çš„æ—¶é—´è¡°å‡å› å­"""
        try:
            # å°è¯•è§£ææ—¶é—´å­—ç¬¦ä¸²
            if 'å°æ—¶å‰' in time_str:
                hours = int(re.search(r'(\d+)', time_str).group(1))
                if hours < 24:
                    return 1.0
                else:
                    return 0.9
            elif 'å¤©å‰' in time_str:
                days = int(re.search(r'(\d+)', time_str).group(1))
                if days == 1:
                    return 0.9
                elif days == 2:
                    return 0.8
                elif days == 3:
                    return 0.7
                elif days <= 7:
                    return 0.6
                elif days <= 14:
                    return 0.4
                elif days <= 30:
                    return 0.2
                else:
                    return 0.1
            else:
                # é»˜è®¤è®¤ä¸ºæ˜¯å½“æ—¥æ–°é—»
                return 1.0
        except:
            return 0.5  # æ— æ³•è§£ææ—¶é—´æ—¶çš„é»˜è®¤å€¼
    
    def _calculate_comprehensive_sentiment(self, sentiment_scores: Dict, 
                                         impact_weights: Dict, 
                                         time_decay_factors: Dict) -> Dict[str, Any]:
        """è®¡ç®—ç»¼åˆæƒ…ç»ªæŒ‡æ ‡"""
        comprehensive = {
            "weighted_sentiment": 0.0,
            "confidence_score": 0.0,
            "sentiment_strength": "å¼±",
            "market_impact_level": "ä½",
            "overall_sentiment": "ä¸­æ€§"
        }
        
        if not sentiment_scores["individual_scores"]:
            return comprehensive
        
        # è®¡ç®—åŠ æƒæƒ…ç»ªåˆ†æ•°
        total_weighted_score = 0.0
        total_weight = 0.0
        
        for i, score_item in enumerate(sentiment_scores["individual_scores"]):
            sentiment = score_item["score"]
            weight = impact_weights["individual_weights"][i]["final_weight"]
            decay = time_decay_factors["individual_factors"][i]["decay_factor"]
            
            final_weight = weight * decay
            total_weighted_score += sentiment * final_weight
            total_weight += final_weight
        
        if total_weight > 0:
            comprehensive["weighted_sentiment"] = total_weighted_score / total_weight
        
        # è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°
        news_count = len(sentiment_scores["individual_scores"])
        avg_weight = impact_weights["average_weight"]
        avg_decay = time_decay_factors["average_decay"]
        
        confidence = min(1.0, (news_count / 10) * avg_weight * avg_decay)
        comprehensive["confidence_score"] = confidence
        
        # åˆ¤æ–­æƒ…ç»ªå¼ºåº¦
        abs_sentiment = abs(comprehensive["weighted_sentiment"])
        if abs_sentiment > 0.6:
            comprehensive["sentiment_strength"] = "å¼º"
        elif abs_sentiment > 0.3:
            comprehensive["sentiment_strength"] = "ä¸­"
        else:
            comprehensive["sentiment_strength"] = "å¼±"
        
        # åˆ¤æ–­å¸‚åœºå½±å“æ°´å¹³
        impact_score = abs_sentiment * confidence
        if impact_score > 0.7:
            comprehensive["market_impact_level"] = "é«˜"
        elif impact_score > 0.4:
            comprehensive["market_impact_level"] = "ä¸­"
        else:
            comprehensive["market_impact_level"] = "ä½"
        
        # åˆ¤æ–­æ€»ä½“æƒ…ç»ª
        weighted_sentiment = comprehensive["weighted_sentiment"]
        if weighted_sentiment > 0.2:
            comprehensive["overall_sentiment"] = "ç§¯æ"
        elif weighted_sentiment < -0.2:
            comprehensive["overall_sentiment"] = "æ¶ˆæ"
        else:
            comprehensive["overall_sentiment"] = "ä¸­æ€§"
        
        return comprehensive
    
    def _analyze_sentiment_trend(self, news_items: List[Dict], sentiment_scores: Dict) -> Dict[str, Any]:
        """åˆ†ææƒ…ç»ªè¶‹åŠ¿"""
        trend = {
            "trend_direction": "ç¨³å®š",
            "trend_strength": 0.0,
            "volatility": 0.0,
            "recent_change": 0.0,
            "trend_description": ""
        }
        
        if len(sentiment_scores["individual_scores"]) < 3:
            return trend
        
        # æŒ‰æ—¶é—´æ’åºï¼ˆç®€åŒ–å¤„ç†ï¼Œå‡è®¾æ–°é—»å·²æŒ‰æ—¶é—´æ’åºï¼‰
        scores = [item["score"] for item in sentiment_scores["individual_scores"]]
        
        # è®¡ç®—è¶‹åŠ¿æ–¹å‘
        recent_scores = scores[-3:]  # æœ€è¿‘3æ¡æ–°é—»
        earlier_scores = scores[:-3] if len(scores) > 3 else scores[:3]
        
        recent_avg = sum(recent_scores) / len(recent_scores)
        earlier_avg = sum(earlier_scores) / len(earlier_scores)
        
        trend["recent_change"] = recent_avg - earlier_avg
        
        if trend["recent_change"] > 0.1:
            trend["trend_direction"] = "ä¸Šå‡"
        elif trend["recent_change"] < -0.1:
            trend["trend_direction"] = "ä¸‹é™"
        else:
            trend["trend_direction"] = "ç¨³å®š"
        
        # è®¡ç®—è¶‹åŠ¿å¼ºåº¦
        trend["trend_strength"] = abs(trend["recent_change"])
        
        # è®¡ç®—æ³¢åŠ¨æ€§
        if len(scores) > 1:
            score_std = np.std(scores)
            trend["volatility"] = score_std
        
        # ç”Ÿæˆè¶‹åŠ¿æè¿°
        if trend["trend_strength"] > 0.3:
            strength_desc = "å¼ºçƒˆ"
        elif trend["trend_strength"] > 0.1:
            strength_desc = "æ˜æ˜¾"
        else:
            strength_desc = "è½»å¾®"
        
        trend["trend_description"] = f"æƒ…ç»ªå‘ˆ{strength_desc}{trend['trend_direction']}è¶‹åŠ¿"
        
        return trend
    
    def _analyze_hot_topics(self, news_items: List[Dict]) -> Dict[str, Any]:
        """åˆ†æçƒ­ç‚¹è¯é¢˜"""
        topics = {
            "keyword_frequency": {},
            "hot_keywords": [],
            "topic_clusters": [],
            "emerging_topics": []
        }
        
        # æå–å…³é”®è¯
        all_text = ""
        for item in news_items:
            all_text += f"{item.get('title', '')} {item.get('content', '')} "
        
        # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPæŠ€æœ¯ï¼‰
        keywords = re.findall(r'[\u4e00-\u9fa5]{2,4}', all_text)
        
        # ç»Ÿè®¡è¯é¢‘
        keyword_count = defaultdict(int)
        for keyword in keywords:
            if len(keyword) >= 2:  # åªè€ƒè™‘é•¿åº¦>=2çš„è¯
                keyword_count[keyword] += 1
        
        # æ’åºå¹¶å–å‰10ä¸ª
        sorted_keywords = sorted(keyword_count.items(), key=lambda x: x[1], reverse=True)
        topics["keyword_frequency"] = dict(sorted_keywords[:20])
        topics["hot_keywords"] = [kw for kw, count in sorted_keywords[:10] if count >= 2]
        
        return topics
    
    def _analyze_market_attention(self, news_items: List[Dict]) -> Dict[str, Any]:
        """åˆ†æå¸‚åœºå…³æ³¨åº¦"""
        attention = {
            "news_volume": len(news_items),
            "attention_level": "ä½",
            "peak_attention_time": None,
            "attention_distribution": {},
            "media_coverage_breadth": 0
        }
        
        # åˆ¤æ–­å…³æ³¨åº¦æ°´å¹³
        if attention["news_volume"] > 20:
            attention["attention_level"] = "é«˜"
        elif attention["news_volume"] > 10:
            attention["attention_level"] = "ä¸­"
        else:
            attention["attention_level"] = "ä½"
        
        # ç»Ÿè®¡åª’ä½“è¦†ç›–å¹¿åº¦
        sources = set(item.get('source', 'æœªçŸ¥') for item in news_items)
        attention["media_coverage_breadth"] = len(sources)
        
        return attention
    
    def _generate_risk_alerts(self, comprehensive_sentiment: Dict, sentiment_trend: Dict) -> List[Dict[str, Any]]:
        """ç”Ÿæˆé£é™©é¢„è­¦"""
        alerts = []
        
        # æç«¯è´Ÿé¢æƒ…ç»ªé¢„è­¦
        if comprehensive_sentiment["weighted_sentiment"] < -0.6:
            alerts.append({
                "type": "æç«¯è´Ÿé¢æƒ…ç»ª",
                "level": "é«˜",
                "description": "æ£€æµ‹åˆ°æç«¯è´Ÿé¢æƒ…ç»ªï¼Œå¯èƒ½å¯¹è‚¡ä»·äº§ç”Ÿé‡å¤§è´Ÿé¢å½±å“",
                "recommendation": "å»ºè®®å¯†åˆ‡å…³æ³¨å¸‚åœºååº”ï¼Œè€ƒè™‘é£é™©æ§åˆ¶æªæ–½"
            })
        
        # æƒ…ç»ªæ€¥å‰§æ¶åŒ–é¢„è­¦
        if sentiment_trend["trend_direction"] == "ä¸‹é™" and sentiment_trend["trend_strength"] > 0.4:
            alerts.append({
                "type": "æƒ…ç»ªæ€¥å‰§æ¶åŒ–",
                "level": "ä¸­",
                "description": "æƒ…ç»ªå‘ˆç°æ€¥å‰§ä¸‹é™è¶‹åŠ¿ï¼Œå¸‚åœºä¿¡å¿ƒå¯èƒ½å—åˆ°å†²å‡»",
                "recommendation": "å»ºè®®å…³æ³¨åç»­æ–°é—»åŠ¨æ€ï¼Œè¯„ä¼°å½±å“æŒç»­æ€§"
            })
        
        # é«˜æ³¢åŠ¨æ€§é¢„è­¦
        if sentiment_trend["volatility"] > 0.5:
            alerts.append({
                "type": "æƒ…ç»ªé«˜æ³¢åŠ¨",
                "level": "ä¸­",
                "description": "æƒ…ç»ªæ³¢åŠ¨æ€§è¾ƒé«˜ï¼Œå¸‚åœºå¯èƒ½é¢ä¸´ä¸ç¡®å®šæ€§",
                "recommendation": "å»ºè®®è°¨æ…æ“ä½œï¼Œæ³¨æ„é£é™©ç®¡ç†"
            })
        
        return alerts
    
    def _assess_news_data_quality(self, news_items: List[Dict]) -> Dict[str, Any]:
        """è¯„ä¼°æ–°é—»æ•°æ®è´¨é‡"""
        quality = {
            "completeness": 0,
            "timeliness": 0,
            "source_reliability": 0,
            "overall_score": 0,
            "quality_level": "æœªçŸ¥"
        }
        
        if not news_items:
            return quality
        
        # è¯„ä¼°å®Œæ•´æ€§
        complete_items = sum(1 for item in news_items 
                           if item.get('title') and item.get('content') and item.get('source'))
        quality["completeness"] = (complete_items / len(news_items)) * 100
        
        # è¯„ä¼°æ—¶æ•ˆæ€§
        fresh_items = sum(1 for item in news_items 
                         if 'å°æ—¶å‰' in item.get('time', '') or 'ä»Šå¤©' in item.get('time', ''))
        quality["timeliness"] = (fresh_items / len(news_items)) * 100
        
        # è¯„ä¼°æ¥æºå¯é æ€§
        reliable_sources = ['å¤®è§†', 'æ–°åç¤¾', 'äººæ°‘æ—¥æŠ¥', 'è¯åˆ¸æ—¶æŠ¥', 'ä¸Šæµ·è¯åˆ¸æŠ¥', 'ä¸­å›½è¯åˆ¸æŠ¥']
        reliable_items = sum(1 for item in news_items 
                           if any(source in item.get('source', '') for source in reliable_sources))
        quality["source_reliability"] = (reliable_items / len(news_items)) * 100
        
        # è®¡ç®—æ€»ä½“è¯„åˆ†
        quality["overall_score"] = (
            quality["completeness"] * 0.4 +
            quality["timeliness"] * 0.3 +
            quality["source_reliability"] * 0.3
        )
        
        if quality["overall_score"] >= 80:
            quality["quality_level"] = "ä¼˜ç§€"
        elif quality["overall_score"] >= 60:
            quality["quality_level"] = "è‰¯å¥½"
        else:
            quality["quality_level"] = "ä¸€èˆ¬"
        
        return quality
    
    def format_enhanced_report(self, analysis_result: Dict[str, Any], ticker: str, company_name: str = "æœªçŸ¥å…¬å¸") -> str:
        """
        æ ¼å¼åŒ–å¢å¼ºæ–°é—»æƒ…ç»ªåˆ†ææŠ¥å‘Š
        
        Args:
            analysis_result: åˆ†æç»“æœå­—å…¸
            ticker: è‚¡ç¥¨ä»£ç 
            company_name: å…¬å¸åç§°
        
        Returns:
            æ ¼å¼åŒ–çš„æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        if "error" in analysis_result:
            return f"# æ–°é—»æƒ…ç»ªåˆ†ææŠ¥å‘Š - {ticker}\n\nâŒ åˆ†æå¤±è´¥: {analysis_result['error']}"
        
        sentiment = analysis_result.get("comprehensive_sentiment", {})
        trend = analysis_result.get("sentiment_trend", {})
        topics = analysis_result.get("hot_topics", {})
        attention = analysis_result.get("market_attention", {})
        alerts = analysis_result.get("risk_alerts", [])
        quality = analysis_result.get("data_quality", {})
        
        report = f"""# {ticker}ï¼ˆ{company_name}ï¼‰å¢å¼ºæ–°é—»æƒ…ç»ªåˆ†ææŠ¥å‘Š

## ğŸ“° é‡åŒ–æƒ…ç»ªæŒ‡æ ‡

### ç»¼åˆæƒ…ç»ªè¯„ä¼°
- **åŠ æƒæƒ…ç»ªåˆ†æ•°**: {sentiment.get('weighted_sentiment', 0):.3f}
- **ç½®ä¿¡åº¦**: {sentiment.get('confidence_score', 0):.3f}
- **æƒ…ç»ªå¼ºåº¦**: {sentiment.get('sentiment_strength', 'æœªçŸ¥')}
- **å¸‚åœºå½±å“æ°´å¹³**: {sentiment.get('market_impact_level', 'æœªçŸ¥')}
- **æ€»ä½“æƒ…ç»ª**: {sentiment.get('overall_sentiment', 'æœªçŸ¥')}

### æƒ…ç»ªè¶‹åŠ¿åˆ†æ
- **è¶‹åŠ¿æ–¹å‘**: {trend.get('trend_direction', 'æœªçŸ¥')}
- **è¶‹åŠ¿å¼ºåº¦**: {trend.get('trend_strength', 0):.3f}
- **æƒ…ç»ªæ³¢åŠ¨æ€§**: {trend.get('volatility', 0):.3f}
- **è¿‘æœŸå˜åŒ–**: {trend.get('recent_change', 0):.3f}
- **è¶‹åŠ¿æè¿°**: {trend.get('trend_description', 'æ— ')}

### å¸‚åœºå…³æ³¨åº¦
- **æ–°é—»æ•°é‡**: {analysis_result.get('news_count', 0)}æ¡
- **å…³æ³¨åº¦æ°´å¹³**: {attention.get('attention_level', 'æœªçŸ¥')}
- **åª’ä½“è¦†ç›–å¹¿åº¦**: {attention.get('media_coverage_breadth', 0)}å®¶åª’ä½“

### çƒ­ç‚¹è¯é¢˜
- **çƒ­é—¨å…³é”®è¯**: {', '.join(topics.get('hot_keywords', [])[:5])}
- **å…³é”®è¯é¢‘æ¬¡**: {dict(list(topics.get('keyword_frequency', {}).items())[:5])}

### é£é™©é¢„è­¦
"""
        
        if alerts:
            for alert in alerts:
                report += f"- **{alert['type']}** ({alert['level']}é£é™©): {alert['description']}\n"
        else:
            report += "- æš‚æ— é£é™©é¢„è­¦\n"
        
        report += f"""

### æ•°æ®è´¨é‡è¯„ä¼°
- **æ•°æ®å®Œæ•´æ€§**: {quality.get('completeness', 0):.1f}%
- **æ•°æ®æ—¶æ•ˆæ€§**: {quality.get('timeliness', 0):.1f}%
- **æ¥æºå¯é æ€§**: {quality.get('source_reliability', 0):.1f}%
- **æ€»ä½“è´¨é‡**: {quality.get('quality_level', 'æœªçŸ¥')}

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {analysis_result.get('analysis_date', 'N/A')}*
*æ•°æ®æ¥æº: å¢å¼ºæ–°é—»æƒ…ç»ªåˆ†æå™¨*
"""
        
        return report


if __name__ == "__main__":
    print("ğŸ“° å¢å¼ºæ–°é—»æƒ…ç»ªåˆ†æå™¨æ¨¡å—")
    print("åŠŸèƒ½: å…ˆé‡åŒ–è®¡ç®—æƒ…ç»ªè¯„åˆ†å’Œå½±å“æƒé‡ï¼Œç„¶åäº¤ç»™LLMè¿›è¡Œæ·±åº¦åˆ†æ")
    print("é€‚ç”¨: è§£å†³DeepSeekç­‰LLMåœ¨æƒ…ç»ªé‡åŒ–ä¸Šä¸å¤Ÿç²¾ç¡®çš„é—®é¢˜")